{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLProject-HP3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY4jGZq43C16"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/DL/final_project/pix2code\""
      ],
      "metadata": {
        "id": "KqykLIsgn4po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of8f7iQDDb4Z",
        "outputId": "2387a4cc-ac28-4e9d-dbab-c1c988a62be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 4\n",
        "embed_size = 512\n",
        "attention_dim = 512\n",
        "decoder_dim = 512\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "alpha_c = 1.\n",
        "grad_clip = 5.\n",
        "encoder_lr = 1e-4\n",
        "decoder_lr = 4e-4\n",
        "\n",
        "shuffle = True\n",
        "num_workers = 2\n",
        "\n",
        "save_after_x_epochs = 2\n",
        "log_step = 1\n",
        "\n",
        "data_dir = base_path+'/datasets/web/processed_data/data_train/'\n",
        "model_path = base_path+'/model/'\n",
        "vocab_path = base_path+'/bootstrap.vocab'\n",
        "\n",
        "crop_size = 224 # Required by resnet152"
      ],
      "metadata": {
        "id": "m9UBv54XDBlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZcD1aqHXqWdK",
        "outputId": "10bb2cc1-c3c8-42ca-b07c-e30cff75aaf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/DL/final_project/pix2code'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "z2hEBQJcoswB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from shutil import copyfile\n",
        "\n",
        "raw_data_dir = data_dir\n",
        "assert os.path.isdir(data_dir)\n",
        "\n",
        "output_dir = base_path+'/datasets/web/processed_data/'\n",
        "filenames = os.listdir(raw_data_dir)\n",
        "\n",
        "filenames = [(f[:-3] + 'gui', f[:-3] + 'png') for f in filenames if f.endswith('.gui')]\n",
        "\n",
        "filenames.sort()\n",
        "random.shuffle(filenames)\n",
        "\n",
        "split_1 = int(0.8 * len(filenames))\n",
        "split_2 = int(0.9 * len(filenames))\n",
        "\n",
        "filenames = {\n",
        "    'train': filenames[:split_1],\n",
        "    'dev': filenames[split_1:split_2],\n",
        "    'test': filenames[split_2:]\n",
        "}\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "else:\n",
        "    print('Warning: output dir {} already exists.'.format(output_dir))\n",
        "\n",
        "for split in ['train', 'dev', 'test']:\n",
        "    output_dir_split = os.path.join(output_dir, 'data_{}'.format(split))\n",
        "    \n",
        "    if not os.path.exists(output_dir_split):\n",
        "        os.mkdir(output_dir_split)\n",
        "    else:\n",
        "        print('Warning: output dir {} already exists.'.format(output_dir_split))\n",
        "        \n",
        "    print('Processing {} data, saving to {}.'.format(split, output_dir_split))\n",
        "    \n",
        "    for (gui, png) in tqdm(filenames[split]):\n",
        "        src_path_gui = os.path.join(raw_data_dir, gui)\n",
        "        output_path_gui = os.path.join(output_dir_split, gui)\n",
        "        src_path_png = os.path.join(raw_data_dir, png)\n",
        "        output_path_png = os.path.join(output_dir_split, png)\n",
        "        \n",
        "        copyfile(src_path_gui, output_path_gui)\n",
        "        copyfile(src_path_png, output_path_png)"
      ],
      "metadata": {
        "id": "R1LD_ezhouYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Vocab"
      ],
      "metadata": {
        "id": "IeK-pHT-FsDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "-YSrvMG5FtEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary (object):\n",
        "    def __init__ (self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "        \n",
        "    def add_word (self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "    \n",
        "    def __call__ (self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "    \n",
        "    def __len__ (self):\n",
        "        return len(self.word2idx)"
      ],
      "metadata": {
        "id": "Adxv6l4MFt00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab (vocab_file_path):\n",
        "    vocab = Vocabulary()\n",
        "\n",
        "    words_raw = load_doc(vocab_file_path)\n",
        "    words = set(words_raw.split(' '))\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "\n",
        "    vocab.add_word(' ')\n",
        "    vocab.add_word('<unk>')     \n",
        "    print('Created vocabulary of ' + str(len(vocab)) + ' items from ' + vocab_file_path)\n",
        "    \n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "_XVt73RCFwXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(vocab_path)\n",
        "\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSKk32g8FyRx",
        "outputId": "4b5d06b8-3856-47dd-a191-7480b8460b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created vocabulary of 19 items from /content/drive/MyDrive/DL/final_project/pix2code/bootstrap.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_pytorch"
      ],
      "metadata": {
        "id": "vka1F05-3DyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bb585d-70a1-4b35-bfe4-f7b27978cfb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit_pytorch\n",
            "  Downloading vit_pytorch-0.35.2-py3-none-any.whl (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 42.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 49.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (0.12.0+cu113)\n",
            "Collecting einops>=0.4.1\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10->vit_pytorch) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit_pytorch) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit_pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit_pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->vit_pytorch) (3.0.4)\n",
            "Installing collected packages: einops, vit-pytorch\n",
            "Successfully installed einops-0.4.1 vit-pytorch-0.35.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Images and Captions"
      ],
      "metadata": {
        "id": "dV9Z0q0CF6aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageHTMLDataSet (Dataset):\n",
        "    def __init__ (self, data_dir, vocab, transform):\n",
        "        self.data_dir = data_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.raw_image_names = []\n",
        "        self.raw_captions = []\n",
        "        \n",
        "        self.filenames = os.listdir(data_dir)\n",
        "        self.filenames.sort()\n",
        "        \n",
        "        for filename in self.filenames:\n",
        "            if filename[-3:] == 'png':\n",
        "                self.raw_image_names.append(filename)\n",
        "            elif filename[-3:] == 'gui':\n",
        "                data = load_doc(data_dir + filename)\n",
        "                self.raw_captions.append(data)\n",
        "                \n",
        "        print('Created dataset of ' + str(len(self)) + ' items from ' + data_dir)\n",
        "\n",
        "    def __len__ (self):\n",
        "        return len(self.raw_image_names)\n",
        "    \n",
        "    def __getitem__ (self, idx):\n",
        "        img_path, raw_caption = self.raw_image_names[idx], self.raw_captions[idx]\n",
        "        \n",
        "        # Get image from filesystem\n",
        "        image = Image.open(os.path.join(self.data_dir, img_path)).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        \n",
        "        # Convert caption (string) to list of vocab ID's\n",
        "        caption = []\n",
        "        caption.append(self.vocab('<START>'))\n",
        "        \n",
        "        # Remove newlines, separate words with spaces\n",
        "        tokens = ' '.join(raw_caption.split())\n",
        "\n",
        "        # Add space after each comma\n",
        "        tokens = tokens.replace(',', ' ,')\n",
        "        \n",
        "        # Split into words\n",
        "        tokens = tokens.split(' ')\n",
        "        \n",
        "        caption.extend([self.vocab(token) for token in tokens])\n",
        "        caption.append(self.vocab('<END>'))\n",
        "        \n",
        "        target = torch.Tensor(caption)\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "# See https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
        "def collate_fn (data):\n",
        "    # Sort datalist by caption length; descending order\n",
        "    data.sort(key = lambda data_pair: len(data_pair[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "    \n",
        "    # Merge images (from tuple of 3D Tensor to 4D Tensor)\n",
        "    images = torch.stack(images, 0)\n",
        "    \n",
        "    # Merge captions (from tuple of 1D tensor to 2D tensor)\n",
        "    lengths = [len(caption) for caption in captions] # List of caption lengths\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    \n",
        "    for i, caption in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = caption[:end]\n",
        "        \n",
        "    return images, targets, lengths"
      ],
      "metadata": {
        "id": "XvHgcoRaF4Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((crop_size, crop_size)), # Match resnet size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "img_html_dataset = ImageHTMLDataSet(data_dir=data_dir, vocab=vocab, transform=transform)\n",
        "data_loader = DataLoader(dataset=img_html_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=shuffle,\n",
        "                         num_workers=num_workers,\n",
        "                         collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "jLY7DK1uGCzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84277f8-416d-466e-b725-eb705df51655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of 1393 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(data_loader))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spcRO0dVS29g",
        "outputId": "ea87ae45-2223-4078-aede-89d16522cd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "csUCWAiQGL1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 5"
      ],
      "metadata": {
        "id": "Q8JmyjkeYF8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=2048):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True) \n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.linear = nn.Linear(in_features = resnet.fc.in_features, out_features = encoded_image_size)\n",
        "        self.bn = nn.BatchNorm1d(num_features = encoded_image_size, momentum = 0.01)\n",
        "      \n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "\n",
        "        out = self.resnet(images)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        out = self.bn(out)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n"
      ],
      "metadata": {
        "id": "Nb56qPF3Ccp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim) \n",
        "        self.full_att = nn.Linear(attention_dim, 1) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1) \n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        att1 = self.encoder_att(encoder_out)  \n",
        "        att2 = self.decoder_att(decoder_hidden) \n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  \n",
        "        alpha = self.softmax(att)  \n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  \n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  \n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  \n",
        "        self.init_weights()  \n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out) \n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  \n",
        "        num_pixels = encoder_out.size(1)\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        embeddings = self.embedding(encoded_captions) \n",
        "\n",
        "        h, c = self.init_hidden_state(encoder_out) \n",
        "\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  \n",
        "            preds = self.fc(self.dropout(h))  \n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "VTU5VvWYaXot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.vit import ViT\n",
        "\n",
        "encoder_vit = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 2048,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "from vit_pytorch.extractor import Extractor\n",
        "v = Extractor(encoder_vit)\n"
      ],
      "metadata": {
        "id": "uusHr3LH3AAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=embed_size,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=0.2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    encoder_vit.cuda()\n",
        "    decoder.cuda()\n",
        "    print('CUDA activated.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laOgGWsUGSl_",
        "outputId": "6eb7b519-9854-49a8-8b7f-cbc71bfe72d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA activated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                             lr=decoder_lr)"
      ],
      "metadata": {
        "id": "Fu7G_kf2PNu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n"
      ],
      "metadata": {
        "id": "mz6bBdQ6OwDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "history = defaultdict()\n",
        "history['loss'], history['perp'],history['time'] = [],[],[]"
      ],
      "metadata": {
        "id": "tyuCMkPZk6CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "REAq4hBFCk1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.train()\n",
        "\n",
        "batch_count = len(data_loader)\n",
        "t0=time.time()\n",
        "for epoch in range(10):\n",
        "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
        "        images = Variable(images.cuda())\n",
        "        captions = Variable(captions.cuda())\n",
        "\n",
        "        lengths = torch.tensor(lengths)\n",
        "        decoder.zero_grad()\n",
        "\n",
        "\n",
        "        _,features = v(images)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, captions, lengths.unsqueeze(1))\n",
        "\n",
        "        targets = captions[:,1:]\n",
        "        scores = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        loss = criterion(scores.data.double(), targets.data)\n",
        "        loss.backward()\n",
        "\n",
        "        clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        \n",
        "        if epoch % log_step == 0 and i == 0:\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['perp'].append(np.exp(loss.item()))\n",
        "            history['time'].append(time.time())\n",
        "            print('Epoch: #%d Loss: %.4f, Perplexity: %5.4f'\n",
        "                  % (epoch, loss.item(), np.exp(loss.item())))\n",
        "            \n",
        "        if (epoch + 1) % save_after_x_epochs == 0 and i == 0:\n",
        "            print('Saving model ' + str(epoch+1))\n",
        "            torch.save(decoder.state_dict(),os.path.join(model_path, 'decoder-hp3-%d-%d.pkl' %(epoch+1, i+1)))\n",
        "            torch.save(encoder_vit.state_dict(), os.path.join(model_path, 'encoder-hp3-%d-%d.pkl' %(epoch+1, i+1)))\n",
        "\n",
        "timeTaken = time.time()-t0\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "z3nu2vYMGW4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccfe265-0711-430d-c6d4-89f4226f5b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #0 Loss: 3.3053, Perplexity: 27.2581\n",
            "Epoch: #1 Loss: 0.3381, Perplexity: 1.4023\n",
            "Saving model 2\n",
            "Epoch: #2 Loss: 0.2226, Perplexity: 1.2493\n",
            "Epoch: #3 Loss: 0.2814, Perplexity: 1.3250\n",
            "Saving model 4\n",
            "Epoch: #4 Loss: 0.2286, Perplexity: 1.2568\n",
            "Epoch: #5 Loss: 0.2093, Perplexity: 1.2328\n",
            "Saving model 6\n",
            "Epoch: #6 Loss: 0.1982, Perplexity: 1.2192\n",
            "Epoch: #7 Loss: 0.2191, Perplexity: 1.2449\n",
            "Saving model 8\n",
            "Epoch: #8 Loss: 0.1900, Perplexity: 1.2092\n",
            "Epoch: #9 Loss: 0.1902, Perplexity: 1.2094\n",
            "Saving model 10\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "SgcPz-o-imgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history['loss'])\n",
        "# plt.plot(history['perp'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "DUHZloAHrpUG",
        "outputId": "2dbc5464-2406-4fe8-815e-30f12706ab1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f159eefcc10>]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['perp'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "z_ryl5Zsr1ty",
        "outputId": "473e52a7-eedd-4b19-d253-c2126509f516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f159ee56150>]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVHklEQVR4nO3db2wb933H8c+XpC3FIp1ZkUxmiVM7mcgg2NakM4JuGYquaYe2G9b2WQO0CIYB6YN2S4cCQ9cn66OhD/pnfbAFcJusAZa1D9oUK4agf5ZlKDoM2ZQ0aJPYrtM4tZPZkjwlkSLFksj77sEdJUqRLFqkdPzdvV+AQPLuyPuClj/30+9+9ztzdwEAwlNIuwAAwM4Q4AAQKAIcAAJFgANAoAhwAAhUaS93NjY25kePHt3LXQJA8J566qlL7j6+cfmeBvjRo0c1OTm5l7sEgOCZ2a82W04XCgAEigAHgEAR4AAQKAIcAAJFgANAoAhwAAgUAQ4AgQoiwJ84Na1/+I8X0i4DAAZKEAH+ny9c0lf/7YxaEXOXA0BbEAFer1W01Ix0bnYx7VIAYGAEEeCNakWSdPrifMqVAMDgCCLAJ6plSdIvpghwAGgLIsAP7C/pptEDtMABoEMQAS5J9WpFp2mBA8CqYAK8USvr7KUFLTVbaZcCAAMhoAA/qFbkenFmIe1SAGAghBPgyUgUTmQCQCyYAD82NqJSwTiRCQCJYAJ8f6mgm8dHaIEDQCKYAJcYiQIAnYIK8Ea1ovOzb2phqZl2KQCQuqACvF7jRCYAtG0b4GZ2xMyeMLPnzew5M7s/Wf55M3vFzJ5Jfj6428UyEgUA1pS62KYp6TPu/rSZVSQ9ZWY/StZ9xd2/uHvlrXfT6AEN7yvo9MU39mqXADCwtg1wd78g6ULyfN7MTkq6YbcL20yhYKpXK7TAAUBX2QduZkcl3SHpyWTRp8zsZ2b2kJkd2uI995nZpJlNzszM9FSsxEgUAGjrOsDNrCzpO5I+7e5zkh6QdIuk2xW30L+02fvc/YS7H3f34+Pj4z0X3KhWNDO/pNmF5Z4/CwBC1lWAm9k+xeH9iLs/KknuPuXuLXePJH1N0p27V+YaRqIAQKybUSgm6UFJJ939yx3Lr+/Y7COSnu1/eW/FSBQAiHUzCuUuSR+X9HMzeyZZ9jlJ95jZ7ZJc0kuSPrErFW5QPTikg8MlnWJOFAA5180olJ9Isk1WPdb/crZnZmrUKvoFAQ4g54K6ErOtUYtHorh72qUAQGrCDPBqRfOXm7o4dzntUgAgNUEGeD05kcnc4ADyLOgAZyQKgDwLMsAPjezX4coQc6IAyLUgA1yKT2TSAgeQZ8EGeHtSq1bESBQA+RRsgDeqFS01I52bXUy7FABIRbgBXmMkCoB8CzbAJ6plSYxEAZBfwQb4gf0l3TR6gLnBAeRWsAEuJScy6UIBkFNBB3ijVtbZSwtaarbSLgUA9lzQAV6vVtSMXGcvLaRdCgDsuaADnJEoAPIs6AC/eaysUsEIcAC5FHSA7y8VdPP4CEMJAeRS0AEuxf3gDCUEkEfBB3ijWtH52Te1sNRMuxQA2FPBB3g9OZF5ZpqpZQHkS/AB3mjf3IETmQByJvgAPzJ6QMP7CvSDA8id4AO8WDBNHObmDgDyJ/gAl+KRKKfoQgGQM5kI8FtrFc3ML2l2YTntUgBgz2QiwNsjUehGAZAnmQjw1ZEoBDiAHMlEgFcPDungcIk5UQDkSiYC3MzUqDESBUC+bBvgZnbEzJ4ws+fN7Dkzuz9ZPmpmPzKzM8njod0vd2v1akWnL87L3dMsAwD2TDct8Kakz7j7bZLeKemTZnabpM9KetzdJyQ9nrxOTaNW0dzlpqbmltIsAwD2zLYB7u4X3P3p5Pm8pJOSbpD0IUkPJ5s9LOnDu1VkN+rJicxTF+fSLAMA9sxV9YGb2VFJd0h6UlLV3S8kqy5Kqm7xnvvMbNLMJmdmZnoo9coYiQIgb7oOcDMrS/qOpE+7+7pmrscdz5t2Prv7CXc/7u7Hx8fHeyr2Sg6N7NfhypBOX2RWQgD50FWAm9k+xeH9iLs/miyeMrPrk/XXS5renRK7x0gUAHnSzSgUk/SgpJPu/uWOVd+TdG/y/F5J/9L/8q5OvVrRmel5tSJGogDIvm5a4HdJ+rik95jZM8nPByV9QdL7zOyMpPcmr1PVqFZ0eSXS+dnFtEsBgF1X2m4Dd/+JJNti9d39Lac37TlRTk/N6+jYSMrVAMDuysSVmG0Th8uSuDsPgHzIVICPDJV0ZPQaneJEJoAcyFSAS1KjepAWOIBcyF6A18o6e2lBS81W2qUAwK7KXIDXqxU1I9fZSwtplwIAuypzAd5oj0ShGwVAxmUuwG8eK6tUMK7IBJB5mQvw/aWCjo2NMCcKgMzLXIBL8QU9tMABZF0mA7xRrejc7KIWlppplwIAuyabAZ6cyDwzTTcKgOzKZoC3b+7ASBQAGZbJAD8yekDD+wo6TT84gAzLZIAXC6aJw5zIBJBtmQxwKb4ik4t5AGRZZgO8UStren5Jry4sp10KAOyKzAZ4nbvUA8i4zAZ4o+PuPACQRZkN8NrBYVWGS/SDA8iszAa4melWLqkHkGGZDXBpbSSKu6ddCgD0XaYDvFGraO5yU1NzS2mXAgB9l+kAb49E4UQmgCzKRYAzJwqALMp0gI+O7Nd4ZYgWOIBMynSAS/HMhIxEAZBFmQ/wehLgrYiRKACyJfMBfmutossrkc7PLqZdCgD0VeYDvM4l9QAyatsAN7OHzGzazJ7tWPZ5M3vFzJ5Jfj64u2Xu3MThsiRGogDInm5a4N+Q9P5Nln/F3W9Pfh7rb1n9MzJU0pHRa2iBA8icbQPc3X8saXYPatk1jEQBkEW99IF/ysx+lnSxHNpqIzO7z8wmzWxyZmamh93tXL1a0YszC1puRqnsHwB2w04D/AFJt0i6XdIFSV/aakN3P+Hux939+Pj4+A5315tGraJm5Dp7aSGV/QPAbthRgLv7lLu33D2S9DVJd/a3rP5qX1J/6uJcypUAQP/sKMDN7PqOlx+R9OxW2w6CW8bLKhWMfnAAmVLabgMz+6akd0saM7OXJf2NpHeb2e2SXNJLkj6xizX2bH+poGNjIzp98Y20SwGAvtk2wN39nk0WP7gLteyqeq2in7/8etplAEDfZP5KzLZGtaJzs4taXG6mXQoA9EVuArx9IvPMFN0oALIhNwHeYE4UABmTmwC/afSAhkoF5kQBkBm5CfBiwTRRLdMCB5AZuQlwSWpUD+o0LXAAGZGvAK+VNT2/pFcXltMuBQB6lqsAX71LPd0oADIgVwHeHolCgAPIglwFeO3gsCrDJU5kAsiEXAW4mcU3d2BOFAAZkKsAl+I5UU5Pzcvd0y4FAHqSuwBvVCt6/c0VTc0tpV0KAPQkfwHOJfUAMiJ3Ab46lJALegAELncBPjqyX+OVIVrgAIKXuwCX4n5wxoIDCF0uA7yeBHgUMRIFQLhyGeCNWlmXVyKdf3Ux7VIAYMdyGeDtE5nMTAggZLkM8AkmtQKQAbkM8PJQSUdGr9EpWuAAApbLAJcYiQIgfLkN8Hq1ohdnFrTcjNIuBQB2JLcB3qhV1IxcZy8tpF0KAOxIbgN8dSQK3SgAApXbAL95fETFgjEnCoBg5TbAh0pFHRsboQUOIFi5DXCJkSgAwrZtgJvZQ2Y2bWbPdiwbNbMfmdmZ5PHQ7pa5Oxq1is7NLmpxuZl2KQBw1bppgX9D0vs3LPuspMfdfULS48nr4NSrFblLZ6a4RyaA8Gwb4O7+Y0mzGxZ/SNLDyfOHJX24z3XtCe7OAyBkO+0Dr7r7heT5RUnVrTY0s/vMbNLMJmdmZna4u91x0+gBDZUKjEQBEKSeT2J6fHv3LSfWdvcT7n7c3Y+Pj4/3uru+KhZME9UyLXAAQdppgE+Z2fWSlDxO96+kvVVnJAqAQO00wL8n6d7k+b2S/qU/5ey9RrWiqbklvba4nHYpAHBVuhlG+E1J/yWpYWYvm9mfSfqCpPeZ2RlJ701eB6lea88NzkgUAGEpbbeBu9+zxaq7+1xLKm5tj0S5OKc7j42mXA0AdC/XV2JKUu3gsCrDJU5kAghO7gPczOJL6i/ShQIgLLkPcCnuBz89Na94RCQAhIEAVzwS5fU3VzQ9v5R2KQDQNQJcHTd34IpMAAEhwCXVq2VJ4oIeAEEhwCVdVx7SWHmIFjiAoBDgiVuTE5kAEAoCPNGeEyWKGIkCIAwEeKJRK+vySqTzry6mXQoAdIUATzASBUBoCPDERLU9qRUBDiAMBHiiPFTSjYeu0WlmJQQQCAK8QzwnCi1wAGEgwDvUaxX9cuYNLTejtEsBgG0R4B1urVXUjFxnLy2kXQoAbIsA77A6EoUTmQACQIB3uHl8RMWC0Q8OIAgEeIehUlHHxkZogQMIAgG+QSO5pB4ABh0BvkG9WtG52UUtLjfTLgUArogA36BRK8tdemGaC3oADDYCfAPmRAEQCgJ8g7ddN6KhUoF+cAADjwDfoFgwTVTLOkULHMCAI8A3UWckCoAAEOCbaFQrmppb0muLy2mXAgBbIsA3Ua+15wZnJAqAwUWAb6LBnCgAAlDq5c1m9pKkeUktSU13P96PotJ2/bXDqgyVmBMFwEDrKcATf+Dul/rwOQPDzFSvVWiBAxhodKFsoVGLR6K4e9qlAMCmeg1wl/RDM3vKzO7bbAMzu8/MJs1scmZmpsfd7Z1GtaLXFlc0Pb+UdikAsKleA/z33f0dkj4g6ZNm9q6NG7j7CXc/7u7Hx8fHe9zd3uGSegCDrqcAd/dXksdpSd+VdGc/ihoE9WpZkrigB8DA2nGAm9mImVXazyX9oaRn+1VY2q4rD2msPEQLHMDA6mUUSlXSd82s/Tn/7O7f70tVA6JRK9MCBzCwdhzg7v6ipLf3sZaBU69W9K3/Pq8ochUKlnY5ALAOwwivoFGt6M2Vll5+9c20SwGAtyDAr6BR45J6AIOLAL+CidWhhHMpVwIAb0WAX0F5qKQbD12j08xKCGAAEeDbaFQrTGoFYCAR4Nuo1yr65cwbWm5GaZcCAOsQ4NtoVCtqRq6X/m8h7VIAYB0CfBvMiQJgUBHg27jl8IiKBeOKTAADhwDfxlCpqGNjI7TAAQwcArwLjSp35wEweAjwLtSrFZ2bXdTicjPtUgBgFQHehUatLHfphWku6AEwOAjwLjASBcAgIsC78LbrRrS/VGAkCoCBQoB3oVgwTRwuMycKgIFCgHeJOVEADBoCvEv1WkUX5y7r9cWVtEsBAEkEeNfaN3f4xTStcACDgQDvUiMZiXKKbhQAA6KXu9LnyvXXDqsyVNJPzszoN8bLGt5X0FCpqOF9BQ3vKyY/8bIiN0AGsAcI8C6ZmX77yLX6wXNT+sFzU1fcdl/RNFwqamg11NeHfLyusG6b4X3Fte06tm8vG2ofKJL3SlIrcjVbrmYUqRm5WpFrpRXFy5N1rWRdvJ2r2epi247XK5Gr1X5vez8d+yyaqVgwlYqmYqGgoknFQkGlgqlYtPix0H4srHtdKGxYXyxs2N5UKhRULOgt712/T5OZ4h+ZCoXkMVmm1efJozq232xZ8rxgJlP8qGRd5zJLjtPt55b8nsSPa783IXF3uUuRu1xafS4lyzrXRZLLFXn8vsjj1+7x+6zjOyvY+n+D9rK19fFor9C+r7QR4FfhgY/9jl6YfkNLK5EuN1taWmnp8kqkyystXV5paakZxa+brWRZpKVmK95+pZUsj/T6mysd74u0lLx3uZX+TSP2FeNw3FcorAZwHKJxYG58HXk77H31YLD++doBo3Nd3qwGujYJeSUHiI5t2weRje/RxmWbbaO1kI2itSDuDNnIJXWEcTucB8FWAb/6urD+gFDsPDgU3vreQfG3H/kt3XlstK+fSYBfhYPD+/SOmw7t2ue3ItdSc+2gEB8Q1g4Gl5ODwVKzJUlrQboarvHrdgiXCoW10O1s3W6x7V51/bivBXrk3tGqby+P1gV+s9WxXRStHjA6DwztcJLarcT1rUOpswWZtDTV2eLsbD2ub312tkJ9i9BbF5Ra29bbT6R169vv2/geuVbr6vyMjZ+rzm02Wd8ZcKt/mXS0iLXJXxO28bXisFTHsvV/rbT/eom36zyotA8l8XcU/ztE3tlaX1vmHv/uX2l9FHW+7vw8VxRt/dmRu9YOa+kaGSr2/TMJ8AFSLJgO7C/pwP60K9ldZslBpP+/z0CuMAoFAAJFgANAoAhwAAgUAQ4AgeopwM3s/WZ22sxeMLPP9qsoAMD2dhzgZlaU9PeSPiDpNkn3mNlt/SoMAHBlvbTA75T0gru/6O7Lkr4l6UP9KQsAsJ1eAvwGSec7Xr+cLFvHzO4zs0kzm5yZmelhdwCATrt+IY+7n5B0QpLMbMbMfrXDjxqTdKlvhYWP72MN38V6fB/rZeH7eNtmC3sJ8FckHel4fWOybEvuPr7TnZnZpLsf3+n7s4bvYw3fxXp8H+tl+fvopQvlfyRNmNkxM9sv6aOSvtefsgAA29lxC9zdm2b2KUk/kFSU9JC7P9e3ygAAV9RTH7i7PybpsT7Vsp0Te7SfUPB9rOG7WI/vY73Mfh/WnpISABAWLqUHgEAR4AAQqCACnDlXYmZ2xMyeMLPnzew5M7s/7ZoGgZkVzeynZvavadeSNjP7NTP7tpmdMrOTZva7adeUFjP7y+T/ybNm9k0zG067pn4b+ABnzpV1mpI+4+63SXqnpE/m+LvodL+kk2kXMSC+Kun77n6rpLcrp9+Lmd0g6S8kHXf331Q8Uu6j6VbVfwMf4GLOlVXufsHdn06ezyv+z/mW6QvyxMxulPRHkr6edi1pM7NrJb1L0oOS5O7L7v5aulWlqiTpGjMrSTog6X9TrqfvQgjwruZcyRszOyrpDklPpltJ6v5O0l9JitIuZAAckzQj6R+TLqWvm9lI2kWlwd1fkfRFSeckXZD0urv/MN2q+i+EAMcGZlaW9B1Jn3b3ubTrSYuZ/bGkaXd/Ku1aBkRJ0jskPeDud0hakJTLc0ZmdkjxX+rHJP26pBEz+1i6VfVfCAF+1XOuZJmZ7VMc3o+4+6Np15OyuyT9iZm9pLhr7T1m9k/plpSqlyW97O7tv8q+rTjQ8+i9ks66+4y7r0h6VNLvpVxT34UQ4My5kjAzU9y/edLdv5x2PWlz97929xvd/aji34t/d/fMtbK65e4XJZ03s0ay6G5Jz6dYUprOSXqnmR1I/t/crQye0N316WR7xZwr69wl6eOSfm5mzyTLPpdMaQBI0p9LeiRp7Lwo6U9TricV7v6kmX1b0tOKR2/9VBm8pJ5L6QEgUCF0oQAANkGAA0CgCHAACBQBDgCBIsABIFAEOAAEigAHgED9P8JCtsKP7UkeAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['time'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "BULK4889IgK3",
        "outputId": "4b3d91c2-a716-48a7-e012-7a50de10f887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f159ee00310>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(base_path+'/history_hp3.json','w') as f:\n",
        "  json.dump(history,f)"
      ],
      "metadata": {
        "id": "-ZrMB-ZAIv9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval"
      ],
      "metadata": {
        "id": "8vvSXlSnGgNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "VgAY2yCpGgoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_idx_to_words (input):\n",
        "    sampled_caption = []\n",
        "    \n",
        "    for idx in input:\n",
        "        word = vocab.idx2word[idx]\n",
        "        sampled_caption.append(word)\n",
        "\n",
        "        if word == '<END>':\n",
        "            break\n",
        "\n",
        "    output = ' '.join(sampled_caption[1:-1])\n",
        "\n",
        "    output = output.replace(' ,', ',')\n",
        "\n",
        "    return output.split(' ')"
      ],
      "metadata": {
        "id": "xqOfwGilGihO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data_dir = base_path+'/datasets/web/processed_data/data_dev/'\n",
        "\n",
        "# Assume format: \"encoder-# epoch-# iter.pkl\"\n",
        "models_to_test = ['2-1','4-1','6-1','8-1','10-1']"
      ],
      "metadata": {
        "id": "uEJ6mc-iGmG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.word2idx"
      ],
      "metadata": {
        "id": "9nc8xAncJtuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "WR5zK9itJ9x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval"
      ],
      "metadata": {
        "id": "yevAL5YT2xOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "\n",
        "for model_idx, model_name in enumerate(models_to_test):\n",
        "    decoder_model_path = os.path.join('/content/drive/MyDrive/DL/final_project/pix2code/model/decoder-hp3-{}.pkl'.format(model_name))\n",
        "    \n",
        "    dev_img_html_dataset = ImageHTMLDataSet(data_dir=dev_data_dir, vocab=vocab, transform=transform)\n",
        "    \n",
        "    dev_data_loader = DataLoader(dataset=dev_img_html_dataset,\n",
        "                             batch_size=1,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=collate_fn)\n",
        "    \n",
        "    dev_encoder = v\n",
        "    dev_decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=embed_size,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=0.2)\n",
        "\n",
        "    dev_decoder.load_state_dict(torch.load(decoder_model_path))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        dev_encoder.cuda()\n",
        "        dev_decoder.cuda()\n",
        "\n",
        "    dev_decoder.eval()\n",
        "    \n",
        "    dev_data_count = len(dev_data_loader.dataset)\n",
        "\n",
        "    predicted, actual = list(), list()\n",
        "    bleu=0.\n",
        "    for i, (images, captions, lengths) in enumerate(dev_data_loader):\n",
        "        images = Variable(images.cuda())\n",
        "\n",
        "        captions = Variable(captions.cuda())\n",
        "        lengths = torch.tensor(lengths)\n",
        "\n",
        "        dev_decoder.zero_grad()\n",
        "        \n",
        "        _,features = dev_encoder(images)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = dev_decoder(features, captions, lengths.unsqueeze(1))\n",
        "\n",
        "        targets = captions[:,1:]\n",
        "\n",
        "        scores = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        predicted = list(np.argmax(scores.data.detach().cpu().numpy(),axis=1))\n",
        "        actual = list(targets.data.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        predicted = [vocab.idx2word[item] for item in predicted]\n",
        "        actual = [vocab.idx2word[item] for item in actual]\n",
        "\n",
        "    \n",
        "        bleu+=corpus_bleu(actual, predicted)\n",
        "    \n",
        "    bleu_scores.append((model_name, bleu/len(dev_data_loader.dataset)))\n",
        "                    \n",
        "    print('done with {} items for model: {}'.format(str(len(predicted)), model_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_LQaeiPK7tu",
        "outputId": "6d381e3a-ffd6-4586-8ed8-cda0b3fcc22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done with 74 items for model: 2-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 74 items for model: 4-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 78 items for model: 6-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 76 items for model: 8-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 78 items for model: 10-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okSMd9IqLMrk",
        "outputId": "21eade82-d6b3-48e3-d36d-bf5b5bae8dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2-1', 0.6727577127730313),\n",
              " ('4-1', 0.6610520548849329),\n",
              " ('6-1', 0.6597872324321675),\n",
              " ('8-1', 0.6577530621330517),\n",
              " ('10-1', 0.6430789998994517)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "ytiEOsd02z8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_dir = base_path+'/datasets/web/processed_data/data_test/'\n",
        "\n",
        "chosen_model = ['2-1']"
      ],
      "metadata": {
        "id": "UpVQCA-y211u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "\n",
        "for model_idx, model_name in enumerate(chosen_model):\n",
        "    decoder_model_path = os.path.join('/content/drive/MyDrive/DL/final_project/pix2code/model/decoder-hp3-{}.pkl'.format(model_name))\n",
        "    \n",
        "    dev_img_html_dataset = ImageHTMLDataSet(data_dir=test_data_dir, vocab=vocab, transform=transform)\n",
        "    \n",
        "    dev_data_loader = DataLoader(dataset=dev_img_html_dataset,\n",
        "                             batch_size=1,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=collate_fn)\n",
        "    \n",
        "    dev_encoder = v\n",
        "    dev_decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=embed_size,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=0.2)\n",
        "\n",
        "    dev_decoder.load_state_dict(torch.load(decoder_model_path))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        dev_encoder.cuda()\n",
        "        dev_decoder.cuda()\n",
        "\n",
        "    dev_decoder.eval()\n",
        "    \n",
        "    dev_data_count = len(dev_data_loader.dataset)\n",
        "\n",
        "    predicted, actual = list(), list()\n",
        "    bleu=0.\n",
        "    for i, (images, captions, lengths) in enumerate(dev_data_loader):\n",
        "        images = Variable(images.cuda())\n",
        "\n",
        "        captions = Variable(captions.cuda())\n",
        "        lengths = torch.tensor(lengths)\n",
        "\n",
        "        dev_decoder.zero_grad()\n",
        "        \n",
        "        _,features = dev_encoder(images)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = dev_decoder(features, captions, lengths.unsqueeze(1))\n",
        "\n",
        "        targets = captions[:,1:]\n",
        "\n",
        "        scores = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        predicted = list(np.argmax(scores.data.detach().cpu().numpy(),axis=1))\n",
        "        actual = list(targets.data.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        predicted = [vocab.idx2word[item] for item in predicted]\n",
        "        actual = [vocab.idx2word[item] for item in actual]\n",
        "\n",
        "    \n",
        "        bleu+=corpus_bleu(actual, predicted)\n",
        "    \n",
        "    bleu_scores.append((model_name, bleu/len(dev_data_loader.dataset)))\n",
        "                    \n",
        "    print('done with {} items for model: {}'.format(str(len(predicted)), model_name))"
      ],
      "metadata": {
        "id": "TJ8nOW6r3jgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47fdeb97-0e95-4ef9-f324-6d333c8fa0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of 175 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_test/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done with 50 items for model: 2-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVVwaSMS_sE4",
        "outputId": "ba64d282-bcdf-47a6-b0cc-36e08bf71fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2-1', 0.6010373674825444)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}