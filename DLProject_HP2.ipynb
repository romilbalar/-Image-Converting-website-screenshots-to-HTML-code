{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLProject-HP2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fb94faf8e784462980a7aa0fa95dfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b282585273ca464383fe6b8048a2840c",
              "IPY_MODEL_4d7787a855744a67a7b639b4f709dc86",
              "IPY_MODEL_8281576c5f3640ec8f1d1b8763031af4"
            ],
            "layout": "IPY_MODEL_6f7e634483264f80b676fc6ae99e13e8"
          }
        },
        "b282585273ca464383fe6b8048a2840c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b3b1df200934c3583b16383895a5ecf",
            "placeholder": "​",
            "style": "IPY_MODEL_6293b6dea90a46de9078bde3b60d771a",
            "value": "100%"
          }
        },
        "4d7787a855744a67a7b639b4f709dc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca20c8af149d49fcadb5987e5178c4ac",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f60a867ec7ff44f68cafc272e7938020",
            "value": 178793939
          }
        },
        "8281576c5f3640ec8f1d1b8763031af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76c2cc2d4ddd45d2bc4f1408474551f7",
            "placeholder": "​",
            "style": "IPY_MODEL_4c4a8bb8070a43f499b5ede9a138de62",
            "value": " 171M/171M [00:00&lt;00:00, 233MB/s]"
          }
        },
        "6f7e634483264f80b676fc6ae99e13e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b3b1df200934c3583b16383895a5ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6293b6dea90a46de9078bde3b60d771a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca20c8af149d49fcadb5987e5178c4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f60a867ec7ff44f68cafc272e7938020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76c2cc2d4ddd45d2bc4f1408474551f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c4a8bb8070a43f499b5ede9a138de62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY4jGZq43C16"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/DL/final_project/pix2code\""
      ],
      "metadata": {
        "id": "KqykLIsgn4po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of8f7iQDDb4Z",
        "outputId": "0a86e17c-ede8-4409-f062-0dc4681b6c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "embed_size = 512\n",
        "attention_dim = 512\n",
        "decoder_dim = 512\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "alpha_c = 1.\n",
        "grad_clip = 5.\n",
        "encoder_lr = 1e-4\n",
        "decoder_lr = 4e-4\n",
        "\n",
        "shuffle = True\n",
        "num_workers = 2\n",
        "\n",
        "save_after_x_epochs = 2\n",
        "log_step = 1\n",
        "\n",
        "data_dir = base_path+'/datasets/web/processed_data/data_train/' \n",
        "model_path = base_path+'/model/'\n",
        "vocab_path = base_path+'/bootstrap.vocab'\n",
        "\n",
        "crop_size = 224 # Required by resnet152"
      ],
      "metadata": {
        "id": "m9UBv54XDBlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZcD1aqHXqWdK",
        "outputId": "0d7d980a-9617-4cbe-f0f6-ab6fea0014a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/DL/final_project/pix2code'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "z2hEBQJcoswB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from shutil import copyfile\n",
        "\n",
        "raw_data_dir = data_dir\n",
        "assert os.path.isdir(data_dir)\n",
        "\n",
        "output_dir = base_path+'/datasets/web/processed_data/'\n",
        "filenames = os.listdir(raw_data_dir)\n",
        "\n",
        "filenames = [(f[:-3] + 'gui', f[:-3] + 'png') for f in filenames if f.endswith('.gui')]\n",
        "\n",
        "random.seed(12345)\n",
        "filenames.sort()\n",
        "random.shuffle(filenames)\n",
        "\n",
        "split_1 = int(0.8 * len(filenames))\n",
        "split_2 = int(0.9 * len(filenames))\n",
        "\n",
        "filenames = {\n",
        "    'train': filenames[:split_1],\n",
        "    'dev': filenames[split_1:split_2],\n",
        "    'test': filenames[split_2:]\n",
        "}\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "else:\n",
        "    print('Warning: output dir {} already exists.'.format(output_dir))\n",
        "\n",
        "for split in ['train', 'dev', 'test']:\n",
        "    output_dir_split = os.path.join(output_dir, 'data_{}'.format(split))\n",
        "    \n",
        "    if not os.path.exists(output_dir_split):\n",
        "        os.mkdir(output_dir_split)\n",
        "    else:\n",
        "        print('Warning: output dir {} already exists.'.format(output_dir_split))\n",
        "        \n",
        "    print('Processing {} data, saving to {}.'.format(split, output_dir_split))\n",
        "    \n",
        "    for (gui, png) in tqdm(filenames[split]):\n",
        "        src_path_gui = os.path.join(raw_data_dir, gui)\n",
        "        output_path_gui = os.path.join(output_dir_split, gui)\n",
        "        src_path_png = os.path.join(raw_data_dir, png)\n",
        "        output_path_png = os.path.join(output_dir_split, png)\n",
        "        \n",
        "        copyfile(src_path_gui, output_path_gui)\n",
        "        copyfile(src_path_png, output_path_png)"
      ],
      "metadata": {
        "id": "R1LD_ezhouYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Vocab"
      ],
      "metadata": {
        "id": "IeK-pHT-FsDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "-YSrvMG5FtEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary (object):\n",
        "    def __init__ (self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "        \n",
        "    def add_word (self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "    \n",
        "    def __call__ (self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "    \n",
        "    def __len__ (self):\n",
        "        return len(self.word2idx)"
      ],
      "metadata": {
        "id": "Adxv6l4MFt00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab (vocab_file_path):\n",
        "    vocab = Vocabulary()\n",
        "\n",
        "    words_raw = load_doc(vocab_file_path)\n",
        "    words = set(words_raw.split(' '))\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "\n",
        "    vocab.add_word(' ')\n",
        "    vocab.add_word('<unk>')\n",
        "    \n",
        "    print('Created vocabulary of ' + str(len(vocab)) + ' items from ' + vocab_file_path)\n",
        "    \n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "_XVt73RCFwXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(vocab_path)\n",
        "\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSKk32g8FyRx",
        "outputId": "0813a248-eb87-4f07-894b-f559a8047528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created vocabulary of 19 items from /content/drive/MyDrive/DL/final_project/pix2code/bootstrap.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Images and Captions"
      ],
      "metadata": {
        "id": "dV9Z0q0CF6aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageHTMLDataSet (Dataset):\n",
        "    def __init__ (self, data_dir, vocab, transform):\n",
        "        self.data_dir = data_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.raw_image_names = []\n",
        "        self.raw_captions = []\n",
        "        \n",
        "        self.filenames = os.listdir(data_dir)\n",
        "        self.filenames.sort()\n",
        "  \n",
        "        for filename in self.filenames:\n",
        "            if filename[-3:] == 'png':\n",
        "\n",
        "                self.raw_image_names.append(filename)\n",
        "            elif filename[-3:] == 'gui':\n",
        "                data = load_doc(data_dir + filename)\n",
        "                self.raw_captions.append(data)\n",
        "                \n",
        "        print('Created dataset of ' + str(len(self)) + ' items from ' + data_dir)\n",
        "\n",
        "    def __len__ (self):\n",
        "        return len(self.raw_image_names)\n",
        "    \n",
        "    def __getitem__ (self, idx):\n",
        "        img_path, raw_caption = self.raw_image_names[idx], self.raw_captions[idx]\n",
        "        \n",
        "\n",
        "        image = Image.open(os.path.join(self.data_dir, img_path)).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        caption = []\n",
        "        caption.append(self.vocab('<START>'))\n",
        "\n",
        "        tokens = ' '.join(raw_caption.split())\n",
        "        tokens = tokens.replace(',', ' ,')\n",
        "        tokens = tokens.split(' ')\n",
        "        \n",
        "        caption.extend([self.vocab(token) for token in tokens])\n",
        "        caption.append(self.vocab('<END>'))\n",
        "        \n",
        "        target = torch.Tensor(caption)\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "def collate_fn (data):\n",
        "    data.sort(key = lambda data_pair: len(data_pair[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "    \n",
        "    images = torch.stack(images, 0)\n",
        "    \n",
        "    lengths = [len(caption) for caption in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    \n",
        "    for i, caption in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = caption[:end]\n",
        "        \n",
        "    return images, targets, lengths"
      ],
      "metadata": {
        "id": "XvHgcoRaF4Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((crop_size, crop_size)), #resnet size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "img_html_dataset = ImageHTMLDataSet(data_dir=data_dir, vocab=vocab, transform=transform)\n",
        "data_loader = DataLoader(dataset=img_html_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=shuffle,\n",
        "                         num_workers=num_workers,\n",
        "                         collate_fn=collate_fn, drop_last=True)"
      ],
      "metadata": {
        "id": "jLY7DK1uGCzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d37d58-671d-4daf-9496-41ab0229c862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of 1393 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "csUCWAiQGL1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=2048):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True) \n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.linear = nn.Linear(in_features = resnet.fc.in_features, out_features = encoded_image_size)\n",
        "        self.bn = nn.BatchNorm1d(num_features = encoded_image_size, momentum = 0.01)\n",
        "      \n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "\n",
        "        out = self.resnet(images)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        out = self.bn(out)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n"
      ],
      "metadata": {
        "id": "PSinwdWtGLd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 5"
      ],
      "metadata": {
        "id": "Q8JmyjkeYF8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMpXqnCqZbvE",
        "outputId": "6dee6ce7-9a79-4cd0-98ae-61b70092aef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  \n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  \n",
        "        self.init_weights()  \n",
        "        \n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out) \n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  \n",
        "        num_pixels = encoder_out.size(1)\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        embeddings = self.embedding(encoded_captions) \n",
        "\n",
        "        h, c = self.init_hidden_state(encoder_out) \n",
        "\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  \n",
        "            preds = self.fc(self.dropout(h))  \n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "VTU5VvWYaXot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=embed_size,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=0.2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "    print('CUDA activated.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "8fb94faf8e784462980a7aa0fa95dfef",
            "b282585273ca464383fe6b8048a2840c",
            "4d7787a855744a67a7b639b4f709dc86",
            "8281576c5f3640ec8f1d1b8763031af4",
            "6f7e634483264f80b676fc6ae99e13e8",
            "5b3b1df200934c3583b16383895a5ecf",
            "6293b6dea90a46de9078bde3b60d771a",
            "ca20c8af149d49fcadb5987e5178c4ac",
            "f60a867ec7ff44f68cafc272e7938020",
            "76c2cc2d4ddd45d2bc4f1408474551f7",
            "4c4a8bb8070a43f499b5ede9a138de62"
          ]
        },
        "id": "laOgGWsUGSl_",
        "outputId": "928d6c98-6b16-47df-a257-02efd1f25789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fb94faf8e784462980a7aa0fa95dfef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA activated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr)\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                             lr=decoder_lr)"
      ],
      "metadata": {
        "id": "Fu7G_kf2PNu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n"
      ],
      "metadata": {
        "id": "mz6bBdQ6OwDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "history = defaultdict()\n",
        "history['loss'], history['perp'],history['time'] = [],[],[]"
      ],
      "metadata": {
        "id": "tyuCMkPZk6CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "m3pyzQkqQjzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "batch_count = len(data_loader)\n",
        "t0=time.time()\n",
        "# history['time'].append(t0)\n",
        "for epoch in range(10):\n",
        "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
        "\n",
        "        images = Variable(images.cuda())\n",
        "        captions = Variable(captions.cuda())\n",
        "\n",
        "        lengths = torch.tensor(lengths)\n",
        "\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "\n",
        "        features = encoder(images) \n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, captions, lengths.unsqueeze(1))\n",
        "\n",
        "        targets = captions[:,1:]\n",
        "        scores = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        loss = criterion(scores.data.double(), targets.data)\n",
        "        loss.backward()\n",
        "\n",
        "        clip_gradient(encoder_optimizer, grad_clip)\n",
        "        clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        \n",
        "        if epoch % log_step == 0 and i == 0:\n",
        "            history['loss'].append(loss.item())\n",
        "            history['perp'].append(np.exp(loss.item()))\n",
        "            history['time'].append(time.time()-t0)\n",
        "            print('Epoch: #%d, Loss: %.4f, Perplexity: %5.4f'\n",
        "                  % (epoch, loss.item(), np.exp(loss.item())))\n",
        "            \n",
        "        if (epoch + 1) % save_after_x_epochs == 0 and i == 0:\n",
        "            # Save our models\n",
        "            print('Saving model ' + str(epoch))\n",
        "            torch.save(decoder.state_dict(),os.path.join(model_path, 'decoder-hp2-%d-%d.pkl' %(epoch+1, i+1)))\n",
        "            torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-hp2-%d-%d.pkl' %(epoch+1, i+1)))\n",
        "\n",
        "timeTaken = time.time()-t0\n",
        "print('done!')"
      ],
      "metadata": {
        "id": "z3nu2vYMGW4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9280f2d0-c7a2-4550-8e57-7083d7ecd208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #0, Loss: 3.0073, Perplexity: 20.2317\n",
            "Epoch: #1, Loss: 0.2453, Perplexity: 1.2781\n",
            "Saving model 1\n",
            "Epoch: #2, Loss: 0.1927, Perplexity: 1.2126\n",
            "Epoch: #3, Loss: 0.1852, Perplexity: 1.2035\n",
            "Saving model 3\n",
            "Epoch: #4, Loss: 0.2012, Perplexity: 1.2229\n",
            "Epoch: #5, Loss: 0.1237, Perplexity: 1.1317\n",
            "Saving model 5\n",
            "Epoch: #6, Loss: 0.1192, Perplexity: 1.1266\n",
            "Epoch: #7, Loss: 0.0958, Perplexity: 1.1005\n",
            "Saving model 7\n",
            "Epoch: #8, Loss: 0.1101, Perplexity: 1.1164\n",
            "Epoch: #9, Loss: 0.0751, Perplexity: 1.0780\n",
            "Saving model 9\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "SgcPz-o-imgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(decoder.state_dict(),'/content/drive/MyDrive/DL/decoder-hp2-%d-%d.pkl' %(epoch+1, i+1))\n",
        "torch.save(encoder.state_dict(), '/content/drive/MyDrive/DL/encoder-hp2-%d-%d.pkl' %(epoch+1, i+1))"
      ],
      "metadata": {
        "id": "agPwJjlobM0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "DUHZloAHrpUG",
        "outputId": "27518f3d-7d5a-41e4-9c5c-844ebbec0df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f69ea222d90>]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['perp'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "z_ryl5Zsr1ty",
        "outputId": "b9ffef15-cade-432c-d192-97011d0f8016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f69ea118bd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaF0lEQVR4nO3dfZAc9Z3f8fdnZnf1sDsCIS0zIGGEjWZUnBMwtcXZwXHhw+aAUOaSXCVQicNdfKX4Cid2ylUX36XqSN39c1VJnOSOiykFFDsVwl1iwx2VwwbKcYpzxWd74cCIBz2ABUjoYZFAz9I+zDd/TI92djWrXe3Mqmd6Pq+q9Xb/+tfdX6a8n2n95jfdigjMzCy7cmkXYGZmS8tBb2aWcQ56M7OMc9CbmWWcg97MLOP60i6gmbVr18aGDRvSLsPMrGs8//zz70XEcLNtHRn0GzZsYHR0NO0yzMy6hqS35trmoRszs4xz0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMm7eoJd0laQfSHpV0iuSvpy0XybpWUk7k9+r59j/vqTPTkn3tfs/oG5iqsp//r+7eG7H2FKdwsysKy3kin4S+GpEXAd8HLhf0nXA14DvR8RG4PvJ+gySLgMeAH4RuAl4YK43hFb15cSW597ku9v2L8Xhzcy61rxBHxH7IuKFZPkY8BqwDrgb+FbS7VvArzTZ/ZeBZyPicES8DzwL3N6OwmeTRLlYYPv+o0txeDOzrnVBY/SSNgAfA34MFCNiX7JpP1Bssss64J2G9T1J25KoFAvsOHAcPzXLzGzagoNe0hDwHeArETHjsjlqydpSukraLGlU0ujY2OLG2SulAsfPTPLukdOtlGJmlikLCnpJ/dRC/tGIeDxpPiDpimT7FcDBJrvuBa5qWF+ftJ0jIrZExEhEjAwPN70B27wqpQIAO/YfW9T+ZmZZtJBZNwIeAV6LiK83bHoSqM+iuQ/48ya7Pw3cJml18iHsbUnbkihfXgv61x30ZmZnLeSK/mbg88AvSXox+bkT+APgs5J2Ap9J1pE0IulhgIg4DPw+8NPk5/eStiVxycp+SquWs+OAg97MrG7e+9FHxA8BzbH51ib9R4HfaFjfCmxdbIEXqlIqsN1X9GZmZ2Xum7GVUoFdY8eZnKqmXYqZWUfIXNCXiwXGJ6vsPnQy7VLMzDpC5oK+Ukxm3nic3swMyGDQbywOIeFxejOzROaCfnl/ng1rBh30ZmaJzAU9QLk45KEbM7NEJoO+Uiyw+9AJTk9MpV2KmVnqshn0pVVUA3YdPJ52KWZmqcto0A8B/kDWzAwyGvRXrxlkIJ/zOL2ZGRkN+v58jg8PD7LdQW9mls2gB9hUKvh2xWZmZDjoy6UC7x45zZFTE2mXYmaWqswGff1WCDs9fGNmPS6zQV9Ogt7j9GbW6zIb9OtXr2BwIO9xejPreZkNekmUSwU/VtDMet68T5iStBW4CzgYER9N2v4UqCRdLgU+iIgbmuy7GzgGTAGTETHSproXpFIs8PQr+4kIao++NTPrPQu5ov8mcHtjQ0T8w4i4IQn37wCPn2f/Tyd9L2rIQ22c/v2TE4wdP3OxT21m1jHmDfqIeA5o+kBv1S6T/wHwWJvraotNpeQhJPt9zxsz612tjtH/beBAROycY3sAz0h6XtLm8x1I0mZJo5JGx8bGWiyrppwE/ev7j7bleGZm3ajVoL+X81/NfzIibgTuAO6X9Km5OkbElogYiYiR4eHhFsuqWTu0jDWDA77njZn1tEUHvaQ+4O8BfzpXn4jYm/w+CDwB3LTY8y1WuVhg+wEP3ZhZ72rliv4zwOsRsafZRkmDkgr1ZeA2YFsL51uUSqnAzgPHqFbjYp/azKwjzBv0kh4DfgRUJO2R9IVk0z3MGraRdKWkp5LVIvBDSS8BPwH+IiK+177SF6ZSKnByfIo975+62Kc2M+sI886jj4h752j/tSZt7wJ3JstvAte3WF/LGm+F8KE1K1Ouxszs4svsN2PrysXa06b8gayZ9arMB31heT/rLl3hxwqaWc/KfNBDbZzeQW9mvaongr5cLPDG2HHGJ6tpl2JmdtH1RNBvKhWYrAa7D51IuxQzs4uuJ4L+7MwbD9+YWQ/qiaD/8PAg+Zwc9GbWk3oi6Jf359mwZqUfK2hmPakngh5gU2mV59KbWU/qmaAvFwu8ffgkJ8cn0y7FzOyi6pmgr5SGiICdvpOlmfWYngn6xnvemJn1kp4J+qvXDLKsL8cOz7wxsx7TM0Gfz4mNxSFf0ZtZz+mZoIfkaVO+ojezHtNTQV8pFjh47AzvnxhPuxQzs4umt4K+VPtA1vPpzayXLORRglslHZS0raHt30jaK+nF5OfOOfa9XdJ2Sbskfa2dhS9GPeg9Tm9mvWQhV/TfBG5v0v4fIuKG5Oep2Rsl5YE/Bu4ArgPulXRdK8W2qrRqOYXlfR6nN7OeMm/QR8RzwOFFHPsmYFdEvBkR48CfAHcv4jhtI4lKseChGzPrKa2M0X9J0s+SoZ3VTbavA95pWN+TtDUlabOkUUmjY2NjLZR1fvWnTUXEkp3DzKyTLDbovwF8BLgB2Af8+1YLiYgtETESESPDw8OtHm5OlVKBo6cn2X/09JKdw8yskywq6CPiQERMRUQV+C/Uhmlm2wtc1bC+PmlLlR9CYma9ZlFBL+mKhtW/C2xr0u2nwEZJ10gaAO4BnlzM+dqpUvQUSzPrLX3zdZD0GHALsFbSHuAB4BZJNwAB7Ab+WdL3SuDhiLgzIiYlfQl4GsgDWyPilSX5r7gAqwcHuLywjO37fRdLM+sN8wZ9RNzbpPmROfq+C9zZsP4UcM7Uy7RVSgW2HziadhlmZhdFT30ztq5cLLDzwHGmqp55Y2bZ15NBXykVODNZ5e3DJ9MuxcxsyfVm0HvmjZn1kJ4M+o3FIcBBb2a9oSeDfuVAHx+6bKWnWJpZT+jJoIf6zBsHvZllX+8GfbHAz987wZnJqbRLMTNbUj0b9OVSgalq8MbBE2mXYma2pHo26H0rBDPrFT0b9NesHaQ/L4/Tm1nm9WzQD/Tl+PDaIXZ4iqWZZVzPBj3Uxulfd9CbWcb1dNBXikPs/eAUx05PpF2KmdmS6e2gL60CYOdB37LYzLKrt4Pe97wxsx7Q00G/fvUKVvTnHfRmlmk9HfS5nCgXhzyX3swybd6gl7RV0kFJ2xra/q2k1yX9TNITki6dY9/dkl6W9KKk0XYW3i6VUsFBb2aZtpAr+m8Ct89qexb4aET8TWAH8Nvn2f/TEXFDRIwsrsSlVS4WeO/4OO8dP5N2KWZmS2LeoI+I54DDs9qeiYjJZPWvgPVLUNtFUSklt0LwOL2ZZVQ7xuj/KfDdObYF8Iyk5yVtPt9BJG2WNCppdGxsrA1lLczZmTcevjGzjGop6CX9a2ASeHSOLp+MiBuBO4D7JX1qrmNFxJaIGImIkeHh4VbKuiDDhWWsXtnvcXozy6xFB72kXwPuAv5RRESzPhGxN/l9EHgCuGmx51sqkigXfSsEM8uuRQW9pNuB3wI+FxEn5+gzKKlQXwZuA7Y165u2SqnAjv3HmOP9ysysqy1keuVjwI+AiqQ9kr4APAgUgGeTqZMPJX2vlPRUsmsR+KGkl4CfAH8REd9bkv+KFpWLBU6MT7H3g1Npl2Jm1nZ983WIiHubND8yR993gTuT5TeB61uq7iLZVJp+CMn61StTrsbMrL16+puxdRuTmTcepzezLHLQA5es6OeKS5Z7Lr2ZZZKDPlEuFth+wLcrNrPscdAnNpUKvHHwOJNT1bRLMTNrKwd9olwsMD5VZfehE2mXYmbWVg76RP2eN9v3e/jGzLLFQZ+49vIhcvI9b8wsexz0ieX9eTasGfTMGzPLHAd9g9rMGwe9mWWLg75BuVRg96ETnJ6YSrsUM7O2cdA32FQqEAG7DvoDWTPLDgd9g3L9ISQepzezDHHQN9iwZiUD+ZzH6c0sUxz0DfryOT5y+ZCv6M0sUxz0s2wqFfxYQTPLFAf9LOVigX1HTnPk1ETapZiZtYWDfpZKaQjAV/VmlhkLCnpJWyUdlLStoe0ySc9K2pn8Xj3HvvclfXZKuq9dhS8Vz7wxs6xZ6BX9N4HbZ7V9Dfh+RGwEvp+szyDpMuAB4BeBm4AH5npD6BTrLl3B0LI+X9GbWWYsKOgj4jng8Kzmu4FvJcvfAn6lya6/DDwbEYcj4n3gWc59w+gokigXh/xYQTPLjFbG6IsRsS9Z3g8Um/RZB7zTsL4naTuHpM2SRiWNjo2NtVBW6yrJzJuISLUOM7N2aMuHsVFLxJZSMSK2RMRIRIwMDw+3o6xFKxcLfHBygrFjZ1Ktw8ysHVoJ+gOSrgBIfh9s0mcvcFXD+vqkraOdfQiJx+nNLANaCfongfosmvuAP2/S52ngNkmrkw9hb0vaOlrFM2/MLEMWOr3yMeBHQEXSHklfAP4A+KykncBnknUkjUh6GCAiDgO/D/w0+fm9pK2jrRlaxtqhAQe9mWVC30I6RcS9c2y6tUnfUeA3Gta3AlsXVV2KykXfCsHMssHfjJ1DbebNcapVz7wxs+7moJ9DpVjg1MQU77x/Mu1SzMxa4qCfQ7nkD2TNLBsc9HOo3/PG4/Rm1u0c9HMYWtbH+tUr2H7Az481s+7moD+PSrHA9v1H0y7DzKwlDvrzKJcKvDl2gvHJatqlmJktmoP+PCrFApPV4OfvnUi7FDOzRXPQn4fveWNmWeCgP48PDw+Sz8nj9GbW1Rz057GsL881awfZvt8zb8ysezno51F/CImZWbdy0M+jUizw9uGTnByfTLsUM7NFcdDPY/obsh6+MbPu5KCfR33mzQ7f88bMupSDfh4fumwly/tznmJpZl1r0UEvqSLpxYafo5K+MqvPLZKONPT53dZLvrjyObHxcn8ga2bda0FPmGomIrYDNwBIylN76PcTTbr+ZUTctdjzdIJyscBzO8fSLsPMbFHaNXRzK/BGRLzVpuN1lEppiLFjZzh8YjztUszMLli7gv4e4LE5tn1C0kuSvivpF9p0vouqUloF+N70ZtadWg56SQPA54D/1WTzC8DVEXE98EfAn53nOJsljUoaHRvrrGGSih9CYmZdrB1X9HcAL0TEgdkbIuJoRBxPlp8C+iWtbXaQiNgSESMRMTI8PNyGstqnuGoZq5b38bqnWJpZF2pH0N/LHMM2kkqSlCzflJzvUBvOeVFJqt0KwUFvZl2opaCXNAh8Fni8oe2Lkr6YrP4qsE3SS8AfAvdERLRyzrRUSgW2HzhGl5ZvZj1s0dMrASLiBLBmVttDDcsPAg+2co5OUSkWOHZ6kn1HTnPlpSvSLsfMbMH8zdgFqt/zxt+QNbNu46BfIN/zxsy6lYN+gS5dOUBx1TJf0ZtZ13HQX4ByscB2X9GbWZdx0F+ASrHAzoPHmap65o2ZdQ8H/QUolwqMT1Z569CJtEsxM1swB/0F2FTyrRDMrPs46C/AtZcPIeFbIZhZV3HQX4CVA3186LKVvqI3s67ioL9AnnljZt3GQX+BNpUK7D50ktMTU2mXYma2IA76C1QuFpiqBm+MHU+7FDOzBXHQX6CKZ96YWZdx0F+ga9YO0p8X2/f7it7MuoOD/gL153N8ZHjIV/Rm1jUc9IvgmTdm1k0c9ItQKRXY+8Epjp2eSLsUM7N5tRz0knZLelnSi5JGm2yXpD+UtEvSzyTd2Oo501Yp1j+Q9Ti9mXW+dl3RfzoiboiIkSbb7gA2Jj+bgW+06Zyp8cwbM+smF2Po5m7gv0XNXwGXSrriIpx3yay7dAUrB/IepzezrtCOoA/gGUnPS9rcZPs64J2G9T1J2wySNksalTQ6NjbWhrKWTi4nNvoDWTPrEu0I+k9GxI3Uhmjul/SpxRwkIrZExEhEjAwPD7ehrKW1qVjw0I2ZdYWWgz4i9ia/DwJPADfN6rIXuKphfX3S1tXKpQKHTozz3vEzaZdiZnZeLQW9pEFJhfoycBuwbVa3J4F/ksy++ThwJCL2tXLeTlCfeePhGzPrdH0t7l8EnpBUP9b/iIjvSfoiQEQ8BDwF3AnsAk4Cv97iOTtCuTQE1IL+5mvXplyNmdncWgr6iHgTuL5J+0MNywHc38p5OtHw0DIuGxzwOL2ZdTx/M3aRJFEuDrHdQW9mHc5B34JKscCO/ceoViPtUszM5uSgb0G5VODE+BR7PziVdilmZnNy0Ldgk2+FYGZdwEHfgo3JFMvXPcXSzDqYg74Fq5b3c+Uly31Fb2YdzUHfonLJ97wxs87moG9RpVTgzbETTExV0y7FzKwpB32LKsUC41NVdr93Iu1SzMyactC3qFy/543H6c2sQznoW3Tt5UPkBDs8Tm9mHcpB36Ll/Xk2rB30Fb2ZdSwHfRtU/LQpM+tgDvo2KBcLvHX4JKfGp9IuxczsHA76NqiUCkTAroPH0y7FzOwcDvo2qJQ888bMOpeDvg2uvmwlA305tu8/mnYpZmbnWHTQS7pK0g8kvSrpFUlfbtLnFklHJL2Y/Pxua+V2pr58jmuHh9h+wEM3ZtZ5WnmU4CTw1Yh4IXlA+POSno2IV2f1+8uIuKuF83SFSqnAj944lHYZZmbnWPQVfUTsi4gXkuVjwGvAunYV1m0qpQL7j57myMmJtEsxM5uhLWP0kjYAHwN+3GTzJyS9JOm7kn7hPMfYLGlU0ujY2Fg7yrqoKr4Vgpl1qJaDXtIQ8B3gKxEx+9PIF4CrI+J64I+AP5vrOBGxJSJGImJkeHi41bIuurJn3phZh2op6CX1Uwv5RyPi8dnbI+JoRBxPlp8C+iWtbeWcnerKS5ZTWNbne96YWcdpZdaNgEeA1yLi63P0KSX9kHRTcr5MfmIpqfYQEl/Rm1mHaWXWzc3A54GXJb2YtP0O8CGAiHgI+FXgNyVNAqeAeyIiWjhnRysXCzz18j4iguT9zcwsdYsO+oj4IXDeNIuIB4EHF3uOblMpDvHYTyY4eOwMxVXL0y7HzAzwN2PbqlJaBeA7WZpZR3HQt1G5OATADo/Tm1kHcdC30ZqhZawdWsbrvqI3sw7ioG+zSmnIV/Rm1lEc9G1WKa5ix4FjVKuZnVxkZl3GQd9mldIQpyeqvPP+ybRLMTMDWptHb02Uk3ve/P1v/D9WDOTpz+foz+Xoy4u+fI7+nOjLi/58jr5c7Xd/Ptmey9Gf14zl2rb6fsn2huXZ2/vyOnu+/rwAUZ/SLzg7v7+2DEpmyDZO+6+3n91vVj/N6L+w41cjiPrvgJi1Pt0OQVBN+tT+YVRfn2d/kvag6f45wUBfjoF8noG+HMv6crX1vhwD+XPX+/K+DrJscNC32d9Ydwm/ectHGDt2hsmpKhPVYHKqyuRUzFg+PjnJRL19qspkNWYsT0xVz26f9DBQKvI5MZBv/mYw+02htpw/u7yssU/jMfpqb/B1avwqimb8qi03vHFOt5273HicZt/Va/wCX30pJ5FTbVtOyXquvt7QVl/ONfaf3q5Z/TRrXzXs26x/7c294U2cWkM1WT67Pfk7iIb26Td4aLwgmLE9+fM554KA6QuD+mvUlxP5XK22fE7kc5xdnm5rWE5es3PbOusLkw76NuvL5/hXt29q6zEjYvqNoJqEf/ImMjFZZbJaZWKq+fb6F5Hj7P80/PGcXWdGv2joXP8jm16OpvtB4/HOPX7jH7lUi6XpP3qAmSHRvJ/O/qtBmmd/6uea7letwvhUlfHJ2s+Zyana8lSVM5PT7fU+Z7fP0efMRJXTE1WOnpqctd/MY2f3u+B2PjPeCJLwb3wjyNffQBq2rx1cxv/84ifaXouDvgtISoZpYAX5tMuxC1B/k258g5iqNrz5NvSr/W52jIZlmr2xNjnOHPvPfgOvRlCtzhwCq8b00NlUdeYwWn37dN/6vo3b6/2bHXtm/6lqzHgzTm6MNfPNmtp2zg4bJhcAuZlDjNPHmHkB0HiM+pv/9IXA9L+Fqsl/b+2/OZiqwlQE1aTt7PKMNs62TVVnbU+Oce5xo6GtYXsEhWVLE8kOerMlNP0mnWNwWdrVWK/yp01mZhnnoDczyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws49SJz+qWNAa8tcjd1wLvtbGcbubXYia/HjP59ZiWhdfi6ogYbrahI4O+FZJGI2Ik7To6gV+Lmfx6zOTXY1rWXwsP3ZiZZZyD3sws47IY9FvSLqCD+LWYya/HTH49pmX6tcjcGL2Zmc2UxSt6MzNr4KA3M8u4zAS9pNslbZe0S9LX0q4nTZKukvQDSa9KekXSl9OuKW2S8pL+WtL/TruWtEm6VNK3Jb0u6TVJ7X92XReR9C+Tv5Ntkh6TtDztmtotE0EvKQ/8MXAHcB1wr6Tr0q0qVZPAVyPiOuDjwP09/noAfBl4Le0iOsR/Ar4XEZuA6+nh10XSOuBfACMR8VEgD9yTblXtl4mgB24CdkXEmxExDvwJcHfKNaUmIvZFxAvJ8jFqf8jr0q0qPZLWA38HeDjtWtIm6RLgU8AjABExHhEfpFtV6vqAFZL6gJXAuynX03ZZCfp1wDsN63vo4WBrJGkD8DHgx+lWkqr/CPwWUE27kA5wDTAG/NdkKOthSYNpF5WWiNgL/DvgbWAfcCQinkm3qvbLStBbE5KGgO8AX4mIo2nXkwZJdwEHI+L5tGvpEH3AjcA3IuJjwAmgZz/TkrSa2r/+rwGuBAYl/eN0q2q/rAT9XuCqhvX1SVvPktRPLeQfjYjH064nRTcDn5O0m9qQ3i9J+u/plpSqPcCeiKj/C+/b1IK/V30G+HlEjEXEBPA48LdSrqntshL0PwU2SrpG0gC1D1OeTLmm1EgStTHY1yLi62nXk6aI+O2IWB8RG6j9/+L/RETmrtgWKiL2A+9IqiRNtwKvplhS2t4GPi5pZfJ3cysZ/HC6L+0C2iEiJiV9CXia2qfmWyPilZTLStPNwOeBlyW9mLT9TkQ8lWJN1jn+OfBoclH0JvDrKdeTmoj4saRvAy9Qm63212Twdgi+BYKZWcZlZejGzMzm4KA3M8s4B72ZWcY56M3MMs5Bb2aWcQ56M7OMc9CbmWXc/wdD5YKIF8aN3wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['time'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ZhDyxqYDRvZ4",
        "outputId": "3e17903c-8248-42e0-8adc-5e534e8ab780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f69ea1039d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "with open(base_path+'/history_hp2.json','w') as f:\n",
        "  json.dump(history,f)"
      ],
      "metadata": {
        "id": "Ybd5oIzeRyc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval"
      ],
      "metadata": {
        "id": "8vvSXlSnGgNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "VgAY2yCpGgoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_idx_to_words (input):\n",
        "    sampled_caption = []\n",
        "    \n",
        "    for idx in input:\n",
        "        word = vocab.idx2word[idx]\n",
        "        sampled_caption.append(word)\n",
        "\n",
        "        if word == '<END>':\n",
        "            break\n",
        "\n",
        "    output = ' '.join(sampled_caption[1:-1])\n",
        "\n",
        "    output = output.replace(' ,', ',')\n",
        "\n",
        "    return output.split(' ')"
      ],
      "metadata": {
        "id": "xqOfwGilGihO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data_dir = base_path+'/datasets/web/processed_data/data_dev/'\n",
        "\n",
        "models_to_test = ['2-1','4-1','6-1','8-1','10-1']"
      ],
      "metadata": {
        "id": "uEJ6mc-iGmG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy Search"
      ],
      "metadata": {
        "id": "C07DvvtcHdQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "\n",
        "for model_idx, model_name in enumerate(models_to_test):\n",
        "    encoder_model_path = os.path.join(model_path, 'encoder-hp2-{}.pkl'.format(model_name))\n",
        "    decoder_model_path = os.path.join(model_path, 'decoder-hp2-{}.pkl'.format(model_name))\n",
        "    \n",
        "    dev_img_html_dataset = ImageHTMLDataSet(data_dir=dev_data_dir, vocab=vocab, transform=transform)\n",
        "    \n",
        "    dev_data_loader = DataLoader(dataset=dev_img_html_dataset,\n",
        "                             batch_size=1,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=collate_fn)\n",
        "    \n",
        "    dev_encoder = Encoder()\n",
        "    dev_decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=embed_size,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=0.2)\n",
        "\n",
        "    dev_encoder.load_state_dict(torch.load(encoder_model_path))\n",
        "    dev_decoder.load_state_dict(torch.load(decoder_model_path))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        dev_encoder.cuda()\n",
        "        dev_decoder.cuda()\n",
        "\n",
        "    dev_encoder.eval()\n",
        "    dev_decoder.eval()\n",
        "    \n",
        "    dev_data_count = len(dev_data_loader.dataset)\n",
        "\n",
        "    predicted, actual = list(), list()\n",
        "    bleu=0.\n",
        "    for i, (images, captions, lengths) in enumerate(dev_data_loader):\n",
        "        images = Variable(images.cuda())\n",
        "\n",
        "        captions = Variable(captions.cuda())\n",
        "\n",
        "        lengths = torch.tensor(lengths)\n",
        "\n",
        "        dev_encoder.zero_grad()\n",
        "        dev_decoder.zero_grad()\n",
        "        \n",
        "        features = dev_encoder(images) \n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = dev_decoder(features, captions, lengths.unsqueeze(1))\n",
        "\n",
        "        targets = captions[:,1:]\n",
        "\n",
        "        scores = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        predicted = list(np.argmax(scores.data.detach().cpu().numpy(),axis=1))\n",
        "        actual = list(targets.data.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        predicted = [vocab.idx2word[item] for item in predicted]\n",
        "        actual = [vocab.idx2word[item] for item in actual]\n",
        "\n",
        "    \n",
        "        bleu+=corpus_bleu(actual, predicted)\n",
        "    \n",
        "    bleu_scores.append((model_name, bleu/len(dev_data_loader.dataset)))\n",
        "                    \n",
        "    print('done with {} items for model: {}'.format(str(len(predicted)), model_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zJ2C__Txa8F",
        "outputId": "65c7556b-ab55-48ff-a5e6-51b2a953c01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done with 76 items for model: 2-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 72 items for model: 4-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 48 items for model: 6-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 78 items for model: 8-1\n",
            "Created dataset of 174 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_dev/\n",
            "done with 74 items for model: 10-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB9LNFBj_bR9",
        "outputId": "5105f019-2807-4d50-a039-9044d8b8ff5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2-1', 0.6821744511881205),\n",
              " ('4-1', 0.6830318112673441),\n",
              " ('6-1', 0.675798928641526),\n",
              " ('8-1', 0.6765190789158899),\n",
              " ('10-1', 0.6771639003037694)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "X-A0REYC0vrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_dir = base_path+'/datasets/web/processed_data/data_test/'\n",
        "\n",
        "chosen_model = ['4-1']"
      ],
      "metadata": {
        "id": "IuhghJMl08KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "\n",
        "for model_idx, model_name in enumerate(chosen_model):\n",
        "    encoder_model_path = os.path.join(model_path, 'encoder-hp2-{}.pkl'.format(model_name))\n",
        "    decoder_model_path = os.path.join(model_path, 'decoder-hp2-{}.pkl'.format(model_name))\n",
        "    \n",
        "    dev_img_html_dataset = ImageHTMLDataSet(data_dir=test_data_dir, vocab=vocab, transform=transform)\n",
        "    \n",
        "    dev_data_loader = DataLoader(dataset=dev_img_html_dataset,\n",
        "                             batch_size=1,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=collate_fn)\n",
        "    \n",
        "    dev_encoder = Encoder()\n",
        "    dev_decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=embed_size,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=0.2)\n",
        "\n",
        "    dev_encoder.load_state_dict(torch.load(encoder_model_path))\n",
        "    dev_decoder.load_state_dict(torch.load(decoder_model_path))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        dev_encoder.cuda()\n",
        "        dev_decoder.cuda()\n",
        "\n",
        "    dev_encoder.eval()\n",
        "    dev_decoder.eval()\n",
        "    \n",
        "    dev_data_count = len(dev_data_loader.dataset)\n",
        "\n",
        "    predicted, actual = list(), list()\n",
        "    bleu=0.\n",
        "    for i, (images, captions, lengths) in enumerate(dev_data_loader):\n",
        "        images = Variable(images.cuda())\n",
        "\n",
        "        captions = Variable(captions.cuda())\n",
        "\n",
        "        lengths = torch.tensor(lengths)\n",
        "\n",
        "        dev_encoder.zero_grad()\n",
        "        dev_decoder.zero_grad()\n",
        "        \n",
        "        features = dev_encoder(images) \n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = dev_decoder(features, captions, lengths.unsqueeze(1))\n",
        "\n",
        "        targets = captions[:,1:]\n",
        "\n",
        "        scores = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        predicted = list(np.argmax(scores.data.detach().cpu().numpy(),axis=1))\n",
        "        actual = list(targets.data.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        predicted = [vocab.idx2word[item] for item in predicted]\n",
        "        actual = [vocab.idx2word[item] for item in actual]\n",
        "\n",
        "    \n",
        "        bleu+=corpus_bleu(actual, predicted)\n",
        "    \n",
        "    bleu_scores.append((model_name, bleu/len(dev_data_loader.dataset)))\n",
        "                    \n",
        "    print('done with {} items for model: {}'.format(str(len(predicted)), model_name))"
      ],
      "metadata": {
        "id": "BQ-ioPE20ysy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7690bd-e63a-4eb2-b00b-b6eb2eaad1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of 175 items from /content/drive/MyDrive/DL/final_project/pix2code/datasets/web/processed_data/data_test/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done with 78 items for model: 4-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVtRL3WlCmEk",
        "outputId": "49afc9f5-4ac7-4e8c-9582-20bce1fef190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('4-1', 0.5088399072211138)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}